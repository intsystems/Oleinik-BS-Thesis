\documentclass{beamer}
\beamertemplatenavigationsymbolsempty
\usecolortheme{beaver}
\setbeamertemplate{blocks}[rounded=true, shadow=true]
\setbeamertemplate{footline}[page number]
%
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{amssymb,amsfonts,amsmath,mathtext}
\usepackage{subfig}
\usepackage[all]{xy} % xy package for diagrams
\usepackage{array}
\usepackage{multicol} % many columns in slide
\usepackage{hyperref} % urls
\usepackage{hhline} %tables
\usepackage{comment} %comments
\newcommand{\blambda}{\boldsymbol{\lambda}}

\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

% Your figures are here:
\graphicspath{ {../figures/} }

%----------------------------------------------------------------------------------------------------------

\title[\hbox to 56mm{Дистилляция знаний в глубоких сетях с применением методов выравнивания структур моделей}]{Дистилляция знаний в глубоких сетях с применением методов выравнивания структур моделей}
\subtitle{\textcolor{black}{Выпускная квалификационная работа бакалавра}}
\author[М.\,С.~Олейник]{
    Михаил Сергеевич Олейник\\
    Научный руководитель: к.ф.-м.н. О.\,Ю.~Бахтеев
}
\institute[МФТИ (НИУ)]{
\small{
    Кафедра интеллектуальных систем ФПМИ МФТИ\\
    Специализация: Интеллектуальный анализ данных\\
    Направление: 01.03.02 Прикладная математика и информатика
}}
\date{2024}


%----------------------------------------------------------------------------------------------------------
\begin{document}
%----------------------------------------------------------------------------------------------------------

\begin{frame}

    \thispagestyle{empty}
    \maketitle

\end{frame}

%-----------------------------------------------------------------------------------------------------

\begin{frame}{Цели исследования}

    \textbf{Проблема}: если модели ученика и учителя имеют сильно отличающиеся архитектуры, то сложно провести дистилляцию знаний.
    Есть методы, с помощью которых это возможно сделать, но они дают малый прирост качества.

    \bigskip

    \textbf{Цель}: предложить метод дистилляции, который будет работать для разных архитектур и с разным количеством слоёв,
    предложить для него алгоритм подбора гиперпараметров.

\end{frame}

%----------------------------------------------------------------------------------------------------------

\begin{frame}{Постановка задачи}
    Дана выборка для задачи классификации на $K$ классов:

    $$\mathfrak{D}  = \{(\bold{x}_i, y_i)\}_{i=1}^{m},\; \bold{x}_i \in \mathbb{R}^n,\; y_i \in \mathbb{Y}  = \{1, \dots, K\},$$


    Обозначим:
    \begin{itemize}
        \item $T$ --- количество слоев в модели учителе
        \item $S$ --- количество слоев в модели ученике
        \item $\bold{t}_{i}$ --- активации в $i$-м слое учителя
        \item $\bold{s}_{i}$ --- активации в $i$-м слое ученика
    \end{itemize}
\end{frame}


%----------------------------------------------------------------------------------------------------------

\begin{frame}{Постановка задачи}
    Функцию потерь ученика представим как:
    $$
        \mathcal{L} = \beta \mathcal{L}_\text{CE} - (1 - \beta){\sum_{i=1}^T \sum_{j=1}^S \lambda_{i, j}I(\bold{t}_{i}, \bold{s}_{j})}
    $$
    Где:
    \begin{itemize}
        \item $\mathcal{L}_\text{CE}$ --- функция потерь для решения задачи классификации (кросс-энтропия),
        \item $I(\bold{t}_{i}, \bold{s}_{j})$ --- взаимная информация,
        \item $\beta$ и $\lambda_{i, j}$ --- гиперпараметры.
    \end{itemize}
\end{frame}

%----------------------------------------------------------------------------------------------------------

\begin{frame}{Схема метода}

    \begin{columns}[c]
        \column{0.4\textwidth}
        \begin{figure}
            \includegraphics[width=1.0\textwidth]{ahn_diagram.pdf}
            \caption{Базовый метод \footnote{Sungsoo Ahn et al. "Variational information distillation for knowledge transfer"}}
        \end{figure}

        \column{0.4\textwidth}
        \begin{figure}
            \includegraphics[width=1.0\textwidth]{our_diagram.pdf}
            \caption{Предлагаемый метод}
        \end{figure}
    \end{columns}

    \begin{equation}
        \mathcal{L} = \beta \mathcal{L}_\text{CE} - (1 - \beta){\sum_{i=1}^T \sum_{j=1}^S \lambda_{i, j}I(\bold{t}_{i}, \bold{s}_{j})}
    \end{equation}


\end{frame}

%----------------------------------------------------------------------------------------------------------

\begin{frame}{Взаимная информация}
    Метод вариации нижней границы:
    \begin{equation}
        I(t, s) = H(t) - H(t|s) \geq  H(t) + E_{t,s}[\log{q(t|s)}].
    \end{equation}
    Вариационное распределение:
    \begin{multline}
        -\log{q(t|s)} = -\sum_{c=1}^{C}  \sum_{h=1}^{H} \sum_{w=1}^{W} \log{q(t_{c,h,w}|s)} = \\
        = \sum_{c=1}^{C}  \sum_{h=1}^{H} \sum_{w=1}^{W} \log{\sigma_c} + \frac{(t_{c,h,w} - \mu_{c,h,w}(s))^2}{2\sigma_c^2} + constant.
    \end{multline}
    Обучаемые параметры:
    $$\sigma^2_c = \log{(1 + e^{\alpha_c})} + \epsilon$$
    $$\mu_{c,h,w}(s) = \mu(s)_{c,h,w}$$
\end{frame}

%-----------------------------------------------------------------------------------------------------

\begin{frame}{Двухуровневая оптимизация}
    $$\mathfrak{D} = \mathfrak{D}_\text{train} \bigsqcup \mathfrak{D}_\text{val}.$$
    Определим вектор $\blambda$ из всех гиперпараметров задачи:
    $$\blambda = [\lambda_{0, 0}, \ldots, \lambda_{i, j}, \ldots, \beta].$$

    Все обучаемые параметры --- $\bold{w}$.

    И это определяет задачу двухуровневой оптимизации:
    $$\min_{\blambda} \quad \mathcal{L}_\text{val}(\bold{\hat{w}}(\blambda), \blambda),$$
    $$\text{s.t.} \quad  \bold{\hat{w}}(\blambda) = \argmin_{\bold{w}}{\mathcal{L}_\text{train}(\bold{w}, \blambda)}. $$

\end{frame}

%-----------------------------------------------------------------------------------------------------

\begin{frame}{Вычислительный эксперимент}
    \begin{columns}[c]
        \column{0.4\textwidth}
        Датасеты:
        \begin{itemize}
            \item CIFAR10
            \item FashionMNIST
        \end{itemize}
        Модели:
        \begin{itemize}
            \item ConvVeryTiny
            \item ConvTiny
        \end{itemize}
        Метрики:
        \begin{itemize}
            \item accuracy
        \end{itemize}
        \column{0.45\textwidth}
        \begin{figure}
            \includegraphics[width=1.0\textwidth]{conv_scheme.png}
            \caption{Схема моделей ConvVeryTiny и ConvTiny}
        \end{figure}
    \end{columns}
\end{frame}

%----------------------------------------------------------------------------------------------------------

\begin{frame}{Эксперимент 1}
    \begin{columns}[c]
        \column{0.4\textwidth}
        \textbf{Учитель}: ConvTiny.

        \textbf{Ученик}: ConvVeryTiny.


        \column{0.55\textwidth}
        \begin{figure}
            \includegraphics[width=1.0\textwidth]{distill_epoch_accuracy.png}
            \caption{Точность от эпохи при дистилляции каждый с каждым}
        \end{figure}
    \end{columns}

    \begin{table}[]
        \begin{tabular}{|l|l|l|l|l|}
            \hline
            Дистилляция & ---  & Хинтона & попарная  & каждый с каждым \\ \hline
            Учитель     & 0.58 & ---     & ---       & ---             \\ \hline
            Ученик      & 0.54 & 0.56    & 0.58-0.59 & 0.58-0.59       \\ \hline
        \end{tabular}
        \caption{Сравнение качества моделей на тестовой выборке}
    \end{table}
\end{frame}

%----------------------------------------------------------------------------------------------------------

\begin{frame}{Эксперимент 1}
    \begin{figure}[H]
        \begin{minipage}[h]{0.35\linewidth}
            \center{\includegraphics[width=1\linewidth]{connections_1}} a) \\
        \end{minipage}
        \hfill
        \begin{minipage}[h]{0.35\linewidth}
            \center{\includegraphics[width=1\linewidth]{connections_2}} b) \\
        \end{minipage}
        \vfill
        \begin{minipage}[h]{0.35\linewidth}
            \center{\includegraphics[width=1\linewidth]{connections_3}} c) \\
        \end{minipage}
        \hfill
        \begin{minipage}[h]{0.35\linewidth}
            \center{\includegraphics[width=1\linewidth]{connections_4}} d) \\
        \end{minipage}
        \caption{Иллюстрация коэффициентов у четырех лучших моделей по качеству. Зелёные точки --- слои ученика, красные --- слои учителя. Чем толще линия, тем больше коэффициент у соответствующей связи.}
    \end{figure}
\end{frame}

%----------------------------------------------------------------------------------------------------------

\begin{frame}{Заключение}
    Был предложен метод дистилляции знаний, который можно применить к моделям с разным количеством слоев и/или разными архитектурами,
    который выдает большее качество, чем дистилляция Хинтона. Однако, к недостаткам данного подхода можно отнести большие требования по памяти и времени,
    если модель учителя и/или ученика имеет большое количество слоёв. В дальнейших планах стоит более тщательное изучение влияния связей между слоями на итоговый результат,
    что потенциально и может свести недостатки подхода к минимуму.
\end{frame}

%-----------------------------------------------------------------------------------------------------

\begin{frame}{Литература}
    \begin{itemize}
        \item Sungsoo Ahn et al. "Variational information distillation for knowledge transfer"
        \item Nikolaos Passalis, Maria Tzelepi, and Anastasios Tefas. "Heterogeneous knowledge distillation using
              information flow modeling"
        \item M Gorpinich, O Yu Bakhteev, and VV Strijov. "Gradient methods for optimizing metaparameters in the
              knowledge distillation problem"
        \item Liu Hanxiao et al. "DARTS: Differentiable Architecture Search"
    \end{itemize}
\end{frame}
%----------------------------------------------------------------------------------------------------------
\end{document}