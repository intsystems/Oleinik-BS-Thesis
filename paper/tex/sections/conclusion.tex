\section{Заключение}

В данной работе исследовалась задача дистилляции в глубоких сетях.
Предложен метод дистилляции, максимизирующий взаимную информацию между отдельными слоями ученика и учителя.
Продемонстрировано, что предложенный метод достигает лучшего качества, чем базовый метод дистилляции знаний.
Дистилляция проводилась с помощью максимизации взаимной информации между всеми слоями ученика и учителя.
Были проведены эксперименты, показавшие эффективность предложенного метода.
Поставлены задачи для дальнейших исследований, а именно выяснить связь между значениями гиперпараметров и качеством модели ученика.
