\section{Постановка задачи}

\subsection{Задача дистилляции с взаимной информацией}

Рассмотрим задачу классификации на $K$ классов:

$$\mathfrak{D}  = \{(\bold{x}_i, y_i)\}_{i=1}^{m},\; \bold{x}_i \in \mathbb{R}^n,\; y_i \in \mathbb{Y}  = \{1, \dots, K\},$$

где $\mathfrak{D}$ это доступная выборка объектов, $y_i$ --- целевая переменная, а $\bold{x}_i$ - данные, описывающие объект, взятые из распределения $p(\bold{x})$.

В задаче дистилляции нам необходима кроме обучаемой модели, так называемого ученика, еще модель учителя,
которая уже предобучена на такой же задаче, и параметры которой не меняются в процессе обучения.
Обозначим $i$-й слой модели учителя как $\mathcal{T}^{(i)}$, и $j$-й слой модели ученика как $\mathcal{S}^{(j)}$. Пропуская же входные данные $\bold{x}$ через модель,
мы можем после каждого слоя получить его активации.
Обозначим активации после $i$-го слоя модели учителя как $\bold{t}^(i)$, а активации после $j$-го слоя модели ученика как $\bold{s}^(j)$.

Взаимная информация пары $(\bold{t}, \bold{s})$ определена как:
$$ I(\bold{t}; \bold{s}) = H(\bold{t}) - H(\bold{t} | \bold{s}) =  -\mathbb{E}_{\bold{t}}[\log{p(\bold{t})}] + \mathbb{E}_{\bold{t},\bold{s}}[\log{p(\bold{t}|\bold{s})}],$$
где энтропия $H(\bold{t})$ и условная энтропия $H(\bold{t}|\bold{s})$ получены из совместного распределения $p(\bold{t},\bold{s})$.
Определение $I(\bold{t}; \bold{s})$ можно понимать, как уменьшение неопределенности в знаниях учителя, которые закодированны в его слое $\bold{t}$,
когда известен слой $\bold{s}$ ученика.

Теперь, можем определить функцию потерь для модели ученика, минимизируя которую ученик будет не только обучаться для задачи классификации,
но и будет максимизироваться взаимная информация между слоями учителя и ученика:
$$\mathcal{L} = \beta \mathcal{L}_\text{CE} - (1 - \beta){\sum_{i=1}^T \sum_{j=1}^S \lambda_{i, j}I(\bold{t}_{i}, \bold{s}_{j})},$$
где $\mathcal{L}_\text{CE}$ -- кросс-энтропия, $T$ --- количество слоёв учителя, $S$ --- количество слоёв ученика, $\beta \in (0;1)$ --- гиперпараметр,
отвечающий за баланс между минимизацией кросс-энтропии и максимизацией взаимной информации между слоями учителя и ученика, $\lambda_{i, j} > 0$ ---
гиперпараметр, отвечающий за важность связи $i$-го слоя учителя и $j$-го слоя ученика.

\subsection{Вариационная максимизация информации}

Функция потерь должна быть минимизирована относительно параметров модели ученика. Однако, сделать это будет сложно, так как трудно вычислить взаимную информацию.

Вместо этого используется вариационная нижняя граница для каждого члена взаимной информации $I(\bold{t}; \bold{s})$,
в которой определяется вариационное распределение $q(\bold{t}|\bold{s})$, которое аппроксимирует $p(\bold{t}|\bold{s})$:

\begin{multline}
    I(\bold{t}; \bold{s}) = H(\bold{t}) - H(\bold{t} | \bold{s}) =  H(\bold{t}) + \mathbb{E}_{\bold{t},\bold{s}}[\log{p(\bold{t}|\bold{s})}] \\
    + H(\bold{t}) + \mathbb{E}_{\bold{t},\bold{s}}[\log{q(\bold{t}|\bold{s})}] + \mathbb{E}_{\bold{s}}[D_{\text{KL}}(p(\bold{t}|\bold{s})||q(\bold{t}|\bold{s}))] \\
    \geq H(\bold{t}) + \mathbb{E}_{\bold{t},\bold{s}}[\log{q(\bold{t}|\bold{s})}].
\end{multline}

Данная техника известна как вариационная максимизация информации. Применяя её к каждому члену взаимной информации в функции потерь, получим:

$$ \mathcal{L} = \beta \mathcal{L}_\text{CE} - (1 - \beta){\sum_{i=1}^T \sum_{j=1}^S \lambda_{i, j} \mathbb{E}_{\bold{t},\bold{s}}[\log{q(\bold{t}^{(i)}|\bold{s}^{(j)})}] }.$$

Эта новая функция потерь минимизируется относительно параметров модели ученика и вариационного распределения $q(\bold{t}|\bold{s})$.
Стоит обратить внимание, что энтропия $H(\bold{t})$ убрана из выражения, так как не зависит от оптимизируемых параметров.

ДОПИСАТЬ О виде $q(\bold{t}|\bold{s})$.

\subsection{Двухуровневая задача оптимизации}

До этого момента мы рассматривали задачу как обычную задачу оптимизации с гиперпараметрами.
Однако, в таком подходе возникают сложности с подбором значений гиперпараметров.
Они могут быть заданы какими-то наивными соображениями, быть подобраны с помощью полного перебора или с использованием вероятностных моделей.
Однако, лучшее качество показывают методы, которые основаны на градиентном спуске, при этом с использованием разностной аппроксимации возможно сохранить
качество, сильно уменьшив сложность метода.

Разобьём нашу выборку на тренировочную и валидационную: $\mathfrak{D} = \mathfrak{D}_\text{train} \bigsqcup \mathfrak{D}_\text{test}.$
Обозначим $\mathcal{L}_\text{train}$ как функцию потерь $\mathcal{L}$, вычисляемую на выборке $\mathfrak{D}_\text{train}$, а $\mathcal{L}_\text{val}$ ---
как  функцию потерь $\mathcal{L}$, вычисляемую на выборке $\mathfrak{D}_\text{val}$.

Определим вектор $\blambda$ из всех гиперпараметров задачи:
$$\blambda = [\lambda_{0, 0}, \ldots, \lambda_{i, j}, \ldots, \beta].$$

Определим все обучаемые параметры модели ученика и обучаемые параметры взаимной информации как $\bold{w}$.

Заметим, что функции потерь $\mathcal{L}_\text{train}$ и $\mathcal{L}_\text{val}$ зависят и от $\blambda$, и от $\bold{w}$.
Цель --- найти $\hat{\blambda}$, которое минимизирует функцию потерь на валидационной выборке $\mathcal{L}_\text{val}(\bold{\hat{w}}, \hat{\blambda})$, где параметры $\bold{w}$
получаются в результате минимизации функции потерь на тренировочной выборке $\bold{\hat{w}} = \argmin_{\bold{w}}{\mathcal{L}_\text{train}(\bold{w}, \hat{\blambda})}$.

И это определяет задачу двухуровневой оптимизации:

$$\min_{\blambda} \quad \mathcal{L}_\text{val}(\bold{\hat{w}}(\blambda), \blambda)$$
$$\text{s.t.} \quad  \bold{\hat{w}}(\blambda) = \argmin_{\bold{w}}{\mathcal{L}_\text{train}(\bold{w}, \blambda)} $$

Используем аппроксимацию (РАСПИСАТЬ ПОДРОБНЕЕ):

$$\nabla_{\blambda}  \mathcal{L}_\text{val}(\bold{\hat{w}}(\blambda), \blambda) \approx \mathcal{L}_\text{val}(\bold{w} - \xi \nabla_{\bold{w}}\mathcal{L}_\text{train}(\bold{w}, \blambda), \blambda),$$
где .... Идея заключается в ....
