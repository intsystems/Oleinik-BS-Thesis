\begin{center}
    \Large{\textbf{Аннотация}}
\end{center}

В данной работе рассматривается задача дистилляции знаний в глубоких сетях.
Методы дистилляции в основном не учитывают разнородность обучаемой модели, так называемого <<ученика>> и обучающей модели, <<учителя>>.
Разнородность моделей ведет к снижению эффективности методов дистилляции и неспособности производить дистилляцию знаний между промежуточными слоями моделей.
При этом даже при похожих архитектурах, но при разном количестве слоёв, существующие методы не учитывают большое количество информации,
которой распологает учитель, что ведёт не к лучшему качеству модели ученика при дистилляции знаний.
Целью работы является предложить метод дистилляции, который будет учитывать больше информации от учителя,
сможет работать с разными архитектурами сетей и показывать лучшее качество, по сравнению с классическими методами.
Предлагается проводить дистилляцию с помощью максимизации взаимной информации между всеми слоями ученика и учителя.
Вводится вариационное распределение, с помощью которого можно максимизировать взаимную информацию,
предлагается его вид для свёрточных и линейных слоёв моделей.
Были проведены эксперименты, показавшие эффективность предложенного метода,
протестированы методы подбора гиперпараметров для данной задачи.
