\begin{center}
    \Large{\textbf{Аннотация}}
\end{center}

Дистилляция знаний позволяет повысить качество модели, называемой учеником, не увеличивая её число параметров,
а используя модель большего размера, называемой учителем.
Однако, в случае разных архитектур и несовпадения количества слоев у учителя и ученика, распространённые методы не применимы.
Одним из подходов, который позволяет решать задачу для разных архитектур, является максимизация взаимной информации.
Мы предлагаем улучшение этого подхода, которое позволит проводить дистилляцию и для моделей с разным количеством слоёв.
Мы сравниваем наш метод с остальными с помощью вычиcлительного эксперимента.
Также проводим анализ гиперпараметров и выводим ограничения на них, при которых достигается наибольшее качество.