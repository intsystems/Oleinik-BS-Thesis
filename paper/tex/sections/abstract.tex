\begin{center}
    \Large{\textbf{Аннотация}}
\end{center}

В данной работе рассматривается задача дистилляции знаний в глубоких сетях.
Текущие методы не позволяют проводить дистилляцию, если архитектуры обучаемой модели,
так называемого <<ученика>>, и модели со знаниями, так называемого <<учителя>>, сильно отличаются.
При этом даже при похожих архитектурах, но при разном количестве слоёв, существующие методы не учитывают большое количество информации,
которой распологает учитель, что ведёт не к лучшему качеству и не эффективной дистилляции знаний.
Целью работы является предложить метод дистилляции, который будет учитывать больше информации от учителя,
сможет работать с разными архитектурами сетей и показывать лучшее качество, по сравнению с классическими методами.
Предлагается проводить дистилляцию с помощью максимизации взаимной информации между всеми слоями ученика и учителя.
Вводится вариационное распределение, с помощью которого можно максимизировать взаимную информацию,
предлагается его вид для свёрточных и линейных слоёв моделей.
Были проведены эксперименты, показавшие эффективность предложенного метода,
протестированы методы подбора гиперпараметров для данной задачи.
