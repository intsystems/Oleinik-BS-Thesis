\section{Введение}

Глубокие нейронные достигли больших успехов в задачах машинного зрения, обработки естественного языка и других.
Однако, лучшие результаты достигают модели с большим количеством параметров,
из-за этого их трудно встроить в системы с небольшими вычислительными мощностями, например, мобильные телефоны.
Если подобрать размер модели под целевую платформу, уменьшив количество параметров, то сильно потеряем и в качестве.

Одним из подходов, которые позволяют не теряя сильно в качестве, получить модель с меньшим количеством параметров, является дистилляция знаний.
Этот подход использует большую предобученную на необходимой задаче модель, называемую учителем,
данные о слоях которой переносятся определенным образом в модель меньшего размера, называемую учеником.
Перенос чаще всего выражается в дополнительном слагаемом в функции потерь ученика.

Первым, и самым классическим методом дистилляции знаний является дистилляция Хинтона, представленная в статье \cite{hinton2015distilling}.
Она заключается в том, что одновременно с обучением ученика основной задаче, например, классификации изображений,
ученик еще старается повторить логиты предобученного учителя, который предобучен на такую же задачу.
У такого подхода много плюсов --- он понятный, не требует большого количества дополнительных ресурсов на вычисления,
и может работать с разными архитектурами, так как на одной и той же задаче, вне зависимости от архитектуры, размерности логитов будут совпадать.
Однако, много информации, которые накопила в себе модель учителя, в таком случае теряется. Можно сказать, что при увеличении размера моделей такая
дистилляция теряет эффективность.

Подходы, которые учитывают большее количество информации от учителя, как правило, показывают лучшее качество \cite{Gou_2021}.
Однако чаще всего они требовательны к архитектуре сетей, и в некоторых задачах их будет сложно или невозможно применить.
Также возникают сложности в процессе дистилляции, если количество параметров в слое учителя сильно больше, чем в соответствующем слое ученика,
как показано в статье \cite{mirzadeh2020improved}.

В настоящее время тема дистилляции знаний активно изучается в мировом сообществе \cite{Gou_2021},
появляются новые методы, в том числе основанные на механизме внимания \cite{passban2020alpkd}.
Наибольший интерес представляют подходы, которые можно применить, если учитель и ученик имеют разные архитектуры.
Так, в статье \cite{passalis2020heterogeneous} происходит дистилляция с помощью моделирования информационного потока в учителе,
и этот поток в процессе обучения старается имитировать ученик.
В статья \cite{Ahn_2019_CVPR} происходит дистилляция знаний для моделей с одинаковым числом слоёв.
Попарно между соответствующими слоями учителя и ученика происходит максимизация взаимной информации.
В основе этого метода используется вариационный подход \cite{barber2004algorithm}.

В некоторых работах \cite{gorpinich2022gradient}  проводится дополнительный анализ и оптимизация гиперпараметров,
которые возникают в задаче дистилляции. Часто используются методы, которые позволяют уменьшить сложность двухуровневой оптимизации,
как например в статье \cite{darts}.

В настоящей работе предлагается улучшение метода, рассмотренного в статье \cite{Ahn_2019_CVPR},
которое состоит в добавлении возможности проведения дистилляции при разном количестве слоев у учителя и ученика.
Схема методов, нашего и базового, изображена на рисунке \ref{fig:method_scheme}.
Актуальность работы заключается в создании метода, который будет использовать большое количество информации от учителя в процессе дистиляции,
будет довольно гибким, подходя под разные архитектуры моделей ученика и учителя, и будет показывать лучшее качество.
