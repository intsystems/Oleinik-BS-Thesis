\section{Введение}

Глубокие нейронные достигли больших успехов в задачах машинного зрения, обработки естественного языка и других.
Однако, лучшие результаты достигают модели с большим количеством параметров,
из-за этого их трудно встроить в системы с небольшими вычислительными мощностями, например, мобильные телефоны.
Если подобрать размер модели под целевую платформу, уменьшив количество параметров, то сильно потеряем и в качестве.

Одним из подходов, которые позволяют не теряя сильно в качестве, получить модель с меньшим количеством параметров, является дистилляция знаний.
Этот подход использует большую предобученную на необходимой задаче модель, называемую учителем,
данные о слоях которой переносятся определенным образом в модель меньшего размера, называемую учеником.
Перенос чаще всего выражается в дополнительном слагаемом в функции потерь ученика.

Так, в работе \cite{hinton2015distilling} предлагается переносить знания с последнего слоя модели.
К недостаткам этого метода можно отнести то, что мы игнорируем информацию из остальных слоев учителя, а она может быть ценной.
В работах ...

Однако, большинство подходов либо неэффективно работают, либо не могут быть применимы к случаям, когда модели имеют разное количество слоёв или разную архитектуру.
Также возникают сложности в случае, когда количество параметров в слое учителя сильно больше,
чем в соответствующем слое ученика, как показано в работе \cite{mirzadeh2020improved}.

Больший интерес представляют подходы, которые можно применить, если учитель и ученик имеют разные архитектуры.
В работе \cite{passalis2020heterogeneous} моделируется информационный поток в учителе, который имитирует ученик.
В работе \cite{Ahn_2019_CVPR} используется максимизация взаимной информации между парами соответствующих слоёв.
В основе этого метода используется вариационный подход \cite{barber2004algorithm}.
Наша работа предлагает улучшение данного метода, давая возможность проведения дистилляции при разном количестве слоев у учителя и ученика.
Также мы проводим анализ гиперпараметров алгоритма, на примере работы \cite{gorpinich2022gradient}.
