\subsection{Двухуровневая задача оптимизации}

До этого момента мы рассматривали задачу как обычную задачу оптимизации с гиперпараметрами.
Однако, в таком подходе возникают сложности с подбором значений гиперпараметров.
Они могут быть заданы какими-то наивными соображениями, быть подобраны с помощью полного перебора или с использованием вероятностных моделей.
Однако, лучшее качество показывают методы, которые основаны на градиентном спуске, при этом с использованием разностной аппроксимации возможно сохранить
качество, сильно уменьшив сложность метода.

Разобьём нашу выборку на тренировочную и валидационную: $\mathfrak{D} = \mathfrak{D}_\text{train} \bigsqcup \mathfrak{D}_\text{test}.$
Обозначим $\mathcal{L}_\text{train}$ как функцию потерь $\mathcal{L}$, вычисляемую на выборке $\mathfrak{D}_\text{train}$, а $\mathcal{L}_\text{val}$ ---
как  функцию потерь $\mathcal{L}$, вычисляемую на выборке $\mathfrak{D}_\text{val}$.

Определим вектор $\blambda$ из всех гиперпараметров задачи:
$$\blambda = [\lambda_{0, 0}, \ldots, \lambda_{i, j}, \ldots, \beta].$$

Определим все обучаемые параметры модели ученика и обучаемые параметры взаимной информации как $\bold{w}$.

Заметим, что функции потерь $\mathcal{L}_\text{train}$ и $\mathcal{L}_\text{val}$ зависят и от $\blambda$, и от $\bold{w}$.
Цель --- найти $\hat{\blambda}$, которое минимизирует функцию потерь на валидационной выборке $\mathcal{L}_\text{val}(\bold{\hat{w}}, \hat{\blambda})$, где параметры $\bold{w}$
получаются в результате минимизации функции потерь на тренировочной выборке $\bold{\hat{w}} = \argmin_{\bold{w}}{\mathcal{L}_\text{train}(\bold{w}, \hat{\blambda})}$.

И это определяет задачу двухуровневой оптимизации:

$$\min_{\blambda} \quad \mathcal{L}_\text{val}(\bold{\hat{w}}(\blambda), \blambda)$$
$$\text{s.t.} \quad  \bold{\hat{w}}(\blambda) = \argmin_{\bold{w}}{\mathcal{L}_\text{train}(\bold{w}, \blambda)} $$

Используем аппроксимацию (РАСПИСАТЬ ПОДРОБНЕЕ):

$$\nabla_{\blambda}  \mathcal{L}_\text{val}(\bold{\hat{w}}(\blambda), \blambda) \approx \mathcal{L}_\text{val}(\bold{w} - \xi \nabla_{\bold{w}}\mathcal{L}_\text{train}(\bold{w}, \blambda), \blambda),$$
где .... Идея заключается в ....
